{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0db63f-8d18-4929-9535-edf58ae15e62",
   "metadata": {},
   "source": [
    "## LLaMA 2 指令微调（Alpaca-Style on Dolly-15K Dataset)\n",
    "\n",
    "示例代码关键训练要素：\n",
    "- 使用 Dolly-15K 数据集，以 Alpaca 指令风格生成训练数据\n",
    "- 以 4-bit（NF4）量化精度加载 `LLaMA 2-7B` 模型\n",
    "- 使用 QLoRA 以 `bf16` 混合精度训练模型\n",
    "- 使用 `HuggingFace TRL` 的 `SFTTrainer` 实现监督指令微调\n",
    "- 使用 Flash Attention 快速注意力机制加速训练（需硬件支持）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97de889-4d1d-4ee3-ad57-289e2e5919d8",
   "metadata": {},
   "source": [
    "### 下载 databricks-dolly-15k 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef235f0d-df0d-4be9-a034-af6684588dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rr-ai/anaconda3/envs/fine_tuning_transformers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, interleave_datasets\n",
    "from random import randrange\n",
    "from torch.utils.data import ConcatDataset \n",
    "# 从hub加载数据集\n",
    "# dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "# dataset = load_dataset(\"/home/rr-ai/huggingface_datasets/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "data_fineval_dir = \"/home/rr-ai/python-project/traindata/fingpt-fineval/\"\n",
    "data_finfq_dir = \"/home/rr-ai/python-project/traindata/fingpt-fiqa_qa/\"\n",
    "\n",
    "dataset_fineval = load_dataset(data_fineval_dir, split=\"train\")\n",
    "dataset_finfq = load_dataset(data_finfq_dir, split=\"train\")\n",
    "\n",
    "# dataset = interleave_datasets([dataset_fineval, dataset_finfq])\n",
    "dataset = ConcatDataset([dataset_fineval, dataset_finfq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9dd8624-871f-4e14-859b-f2ec1e1b793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from random import randrange\n",
    "\n",
    "# 从hub加载数据集\n",
    "# dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "# dataset = load_dataset(\"/home/rr-ai/huggingface_datasets/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "data_fineval_dir = \"/home/rr-ai/python-project/traindata/fingpt-fineval/\"\n",
    "data_finfq_dir = \"/home/rr-ai/python-project/traindata/fingpt-fiqa_qa/\"\n",
    "\n",
    "dataset_fineval = load_dataset(data_fineval_dir, split=\"train\")\n",
    "dataset_finfq = load_dataset(data_finfq_dir, split=\"train\")\n",
    "\n",
    "dataset = concatenate_datasets([dataset_fineval, dataset_finfq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c5f398a-6dd4-4f71-a474-1b647103471a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'instruction'],\n",
       "    num_rows: 18166\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据集样例总数: 15011\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c800fe8-59b2-4cc7-be3d-91f1b411f123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'instruction'],\n",
       "    num_rows: 17110\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_finfq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9774de6d-e0a1-497b-ac6e-8d11be0b0395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '1848年，芝加哥的82位商人发起组建了____。\\nA. 芝加哥期权交易所(CBOE)\\nB. 芝加哥期货交易所(CBOT)\\nC. 芝加哥股票交易所(CHX)\\nD. 芝加哥商业交易所(CME)\\n',\n",
       " 'output': 'B. 芝加哥期货交易所(CBOT)',\n",
       " 'instruction': '以下是中国关于期货从业资格证考试的单项选择题，请选出其中的正确答案。'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fineval[1054]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c5990b5-9b17-482b-95b6-32b0aa1e97c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Do I need to own all the funds my target-date funds owns to mimic it?',\n",
       " 'output': 'If you read Joel Greenblatt\\'s The Little Book That Beats the Market, he says: Owning two stocks eliminates 46% of the non market risk of owning just one stock.    This risk is reduced by 72% with 4 stocks, by 81% with 8 stocks, by 93% with 16 stocks, by 96% with 32 stocks, and by 99% with 500 stocks.  Conclusion: After purchasing 6-8 stocks, benefits of adding stocks to decrease risk are small.   Overall market risk won\\'t be eliminated merely by adding more stocks.  And that\\'s just specific stocks. So you\\'re very right that allocating a 1% share to a specific type of fund is not going to offset your other funds by much. You are correct that you can emulate the lifecycle fund by simply buying all the underlying funds, but there are two caveats: Generally, these funds are supposed to be cheaper than buying the separate funds individually. Check over your math and make sure everything is in order. Call the fund manager and tell him about your findings and see what they have to say. If you are going to emulate the lifecycle fund, be sure to stay on top of rebalancing. One advantage of buying the actual fund is that the portfolio distributions are managed for you, so if you\\'re going to buy separate ETFs, make sure you\\'re rebalancing. As for whether you need all those funds, my answer is a definite no. Consider Mark Cuban\\'s blog post Wall Street\\'s new lie to Main Street - Asset Allocation. Although there are some highly questionable points in the article, one portion is indisputably clear:  Let me translate this all for you. “I want you to invest 5pct in cash and the rest in 10 different funds about which you know absolutely nothing. I want you to make this investment knowing that even if there were 128 hours in a day and you had a year long vacation, you could not possibly begin to understand all of these products. In fact, I don’t understand them either, but because I know it sounds good and everyone is making the same kind of recommendations, we all can pretend we are smart and going to make a lot of money. Until we don’t\" Standard theory says that you want to invest in low-cost funds (like those provided by Vanguard), and you want to have enough variety to protect against risk. Although I can\\'t give a specific allocation recommendation because I don\\'t know your personal circumstances, you should ideally have some in US Equities, US Fixed Income, International Equities, Commodities, of varying sizes to have adequate diversification \"as defined by theory.\" You can either do your own research to establish a distribution, or speak to an investment advisor to get help on what your target allocation should be.',\n",
       " 'instruction': 'Offer your thoughts or opinion on the input financial query or topic using your financial background.'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[18100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61788bc1-6c19-4301-9089-14a59c23b869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What time period is used by yahoo finance to calculate beta', 'output': 'Citing the Yahoo Finance Help page, Beta: The Beta used is Beta of Equity. Beta is the monthly price   change of a particular company relative to the monthly price change of   the S&P500. The time period for Beta is 3 years (36 months) when   available. Regarding customised time periods, I do not think so.', 'instruction': 'Offer your thoughts or opinion on the input financial query or topic using your financial background.'}\n"
     ]
    }
   ],
   "source": [
    "# 随机抽选一个数据样例打印\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71040ca1-92d0-4e1e-af66-49ef2a52c3d7",
   "metadata": {},
   "source": [
    "### 以 Alpaca-Style 格式化指令数据\n",
    "\n",
    "`Alpacca-style` 格式：https://github.com/tatsu-lab/stanford_alpaca#data-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d2889f7-ff56-4ee5-a852-1794c0a296ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_instruction(sample_data):\n",
    "#     \"\"\"\n",
    "#     Formats the given data into a structured instruction format.\n",
    "\n",
    "#     Parameters:\n",
    "#     sample_data (dict): A dictionary containing 'response' and 'instruction' keys.\n",
    "\n",
    "#     Returns:\n",
    "#     str: A formatted string containing the instruction, input, and response.\n",
    "#     \"\"\"\n",
    "#     # Check if required keys exist in the sample_data\n",
    "#     if 'response' not in sample_data or 'instruction' not in sample_data:\n",
    "#         # Handle the error or return a default message\n",
    "#         return \"Error: 'response' or 'instruction' key missing in the input data.\"\n",
    "\n",
    "#     return f\"\"\"### Instruction:\n",
    "# Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    " \n",
    "# ### Input:\n",
    "# {sample_data['response']}\n",
    " \n",
    "# ### Response:\n",
    "# {sample_data['instruction']}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "806c968a-46e4-4620-9cfe-127989c77948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample_data):\n",
    "    \"\"\"\n",
    "    Formats the given data into a structured instruction format.\n",
    "\n",
    "    Parameters:\n",
    "    sample_data (dict): A dictionary containing 'response' and 'instruction' keys.\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted string containing the instruction, input, and response.\n",
    "    \"\"\"\n",
    "    # Check if required keys exist in the sample_data\n",
    "    if 'input' not in sample_data or 'output' not in sample_data or 'instruction' not in sample_data:\n",
    "        # Handle the error or return a default message\n",
    "        return \"Error: 'response' or 'instruction' key missing in the input data.\"\n",
    "\n",
    "    return f\"\"\"### Instruction:\n",
    "{sample_data['instruction']} \n",
    " \n",
    "### Input:\n",
    "{sample_data['input']}\n",
    " \n",
    "### Response:\n",
    "{sample_data['output']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26167b60-f339-487a-a2dc-0cf49fac8932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Based on your financial expertise, provide your response or viewpoint on the given financial question or topic. The response format is open. \n",
      " \n",
      "### Input:\n",
      "What's the purpose of having separate checking and savings accounts?\n",
      " \n",
      "### Response:\n",
      "A checking account is instant access. It can be tapped via check or debit card.  A savings account is supposed to be used to accumulate cash for a goal that is is longer term or for an emergency.  Many people need to separate these funds into different accounts to be able to know if they are overspending or falling short on their savings. In the United States the Federal Reserve also looks at these accounts differently. Money in a checking account generally can't be used to fund loans, money in a savings account can be used as a source of loans by the bank. An even greater percentage of funds in longer term accounts can be used to fund loans. This includes Certificates of Deposit, and retirement accounts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 随机抽选一个样例，打印 Alpaca 格式化后的样例 \n",
    "print(format_instruction(dataset[randrange(len(dataset))]))\n",
    "# print(format_instruction(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b77d2-58e2-4018-a943-51bbfd68ee4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eea57c8-ffac-422c-9fa1-b4b98dd3f917",
   "metadata": {},
   "source": [
    "### 使用快速注意力（Flash Attention）加速训练\n",
    "\n",
    "检查你的 GPU 是否支持 `flash-attn` 加速：\n",
    "\n",
    "```shell\n",
    "$ python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\"\n",
    "\n",
    "Traceback (most recent call last):\n",
    "  File \"<string>\", line 1, in <module>\n",
    "AssertionError: Hardware not supported for Flash Attention\n",
    "```\n",
    "**运行结果：演示使用的 NVIDIA T4 硬件不支持 Flash Attention**\n",
    "\n",
    "#### 安装 flash-attn 加速包（需要GPU硬件支持）\n",
    "\n",
    "```shell\n",
    "$ MAX_JOBS=4 pip install flash-attn --no-build-isolation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a715dc-3ecd-41bb-9b2e-88c1db991e92",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "677fa03d-439e-4aa1-81f0-e1015d71c189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.32s/it]\n",
      "/home/rr-ai/anaconda3/envs/fine_tuning_transformers/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/rr-ai/anaconda3/envs/fine_tuning_transformers/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# 如果硬件设备支持，成功安装 flash-attn后，将 use_flash_attention 设置为True\n",
    "use_flash_attention = False\n",
    " \n",
    "# 取消注释以使用 flash-atten\n",
    "# if torch.cuda.get_device_capability()[0] >= 8:\n",
    "#     from utils.llama_patch import replace_attn_with_flash_attn\n",
    "#     print(\"Using flash attention\")\n",
    "#     replace_attn_with_flash_attn()\n",
    "#     use_flash_attention = True\n",
    " \n",
    " \n",
    "# 获取 LLaMA 2-7B 模型权重\n",
    "# 无需 Meta AI 审核的模型权重\n",
    "# model_id = \"NousResearch/Llama-2-7b-hf\" \n",
    "# 通过 Meta AI 审核后可使用此 Model ID 下载\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" \n",
    "# model_id = '/home/rr-ai/huggingface_models/Llama-2-7b-hf/' \n",
    "model_id = '/home/rr-ai/huggingface_models/Llama2-Chinese-13b-Chat/' \n",
    "\n",
    " \n",
    "# 使用 BnB 加载量化后的模型\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    " \n",
    "# 加载模型与分词器\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, use_cache=True, device_map=\"auto\")\n",
    "model.config.pretraining_tp = 1 \n",
    " \n",
    "# 通过对比doc中的字符串，验证模型是否在使用flash attention\n",
    "if use_flash_attention:\n",
    "    from utils.llama_patch import forward    \n",
    "    assert model.model.layers[0].self_attn.forward.__doc__ == forward.__doc__, \"Model is not using flash attention\"\n",
    " \n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b1b42-97f2-4378-99d3-d582fae48e03",
   "metadata": {},
   "source": [
    "### 使用 QLoRA 配置加载 PEFT 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50690552-1807-4e53-892a-1aad10cadaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    " \n",
    "# QLoRA 配置\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\", \n",
    ")\n",
    " \n",
    " \n",
    "# 使用 QLoRA 配置加载 PEFT 模型\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "qlora_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98490c6a-fa9d-460b-8362-33444efd2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,107,200 || all params: 13,028,971,520 || trainable%: 0.10060041945659269\n"
     ]
    }
   ],
   "source": [
    "qlora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0f7b6-30a3-4e10-8a4a-9840e465e590",
   "metadata": {},
   "source": [
    "### 训练超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb427d32-575f-4af7-af71-bbcfc0b3b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 演示训练参数（实际训练是设置为 False）\n",
    "demo_train = False\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "output_dir = f\"models/Llama2-Chinese-13b-Chat-int4-finfq-{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f57027d-63cc-4199-9ae7-d04fa9db3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    " \n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1 if demo_train else 3,\n",
    "    max_steps=450,\n",
    "    per_device_train_batch_size=2, # Nvidia T4 16GB 显存支持的最大 Batch Size\n",
    "    gradient_accumulation_steps=1 if demo_train else 4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\" if demo_train else \"epoch\",\n",
    "    save_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e28e91-6daa-44c9-8e1e-59abddd57697",
   "metadata": {},
   "source": [
    "### 实例化 SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53c29ebe-f962-4b2f-9905-743f63024cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2762 examples [00:01, 1894.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    " \n",
    "# 数据集的最大长度序列（筛选后的训练数据样例数为1158）\n",
    "max_seq_length = 2048 \n",
    " \n",
    "trainer = SFTTrainer(\n",
    "    model=qlora_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction, \n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716498f-93c7-4071-858e-c53d270eeb2d",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43d07ef2-d07c-41c1-8eaa-2472f289055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/rr-ai/anaconda3/envs/fine_tuning_transformers/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 2:07:16, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.941300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.818500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.787700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.797100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.785900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.762300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.756100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.752900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.725300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.708200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.700300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.721200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.614700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.753400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.758400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.758700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.764500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.732600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.735500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.772600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.727500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.647200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.754100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.720500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.716900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.656400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.671400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.737800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.677700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.715200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rr-ai/anaconda3/envs/fine_tuning_transformers/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=1.7406717427571614, metrics={'train_runtime': 7654.1957, 'train_samples_per_second': 0.47, 'train_steps_per_second': 0.059, 'total_flos': 5.69112250023936e+17, 'train_loss': 1.7406717427571614, 'epoch': 1.3})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5eeb5e-2082-4ead-b4a2-ebedb06333f9",
   "metadata": {},
   "source": [
    "### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e874176b-1b1a-4264-b24e-5308c773e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39256f7-6b10-4f6d-8a99-c96a512604ec",
   "metadata": {},
   "source": [
    "### 模型推理（测试）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e38d38d-a675-40a1-aa0d-b78e90865162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
